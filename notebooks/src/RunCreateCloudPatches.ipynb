{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17122ab3-9b89-432f-8d96-2e17017e32c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    from common.sagemaker_env import LABELBOX_API_KEY\n",
    "except: \n",
    "    LABELBOX_API_KEY = os.environ['LABELBOX_API_KEY']\n",
    "    \n",
    "    \n",
    "\n",
    "CLOUD_PROJECT_ID = \"cleamnf3q398707ug5s2z4rp6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5ad48611-dca8-4138-8709-dedc93b87bb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "labelbox_dir = \"./data/labelbox\"\n",
    "clouds_dir = f'{labelbox_dir}/clouds'\n",
    "source_dir = f'{clouds_dir}/source'\n",
    "training_dir = f'{clouds_dir}/training'\n",
    "labels_dir = f'{training_dir}/labels'\n",
    "rgb_dir = f'{clouds_dir}/rgb'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "524df513-954f-4c6b-8e89-b7598bfc7e9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def is_island(target_polygon, polygon_list):\n",
    "    for curr_poly in polygon_list:\n",
    "        if target_polygon.within(curr_poly):\n",
    "            return True\n",
    "        \n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e48639-511e-40e4-9c44-4652475d0012",
   "metadata": {},
   "source": [
    "## OLD - use parallel version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dec813a-c2ad-4d09-a2b0-af6707921d13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S2A_35MQV_20200731_0_L2A\n",
      "S2A_35MQV_20200820_0_L2A\n",
      "S2A_35MQU_20211123_0_L2A\n",
      "S2B_35MQU_20211108_0_L2A\n",
      "S2B_35MQU_20211029_0_L2A\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16/840083218.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mbbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_max\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0mimagery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_array_to_tif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgb_data_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrgb_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_cog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m             \u001b[0mimagery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_array_to_tif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNODATA_FLOAT32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data/smartcarte-containers/notebooks/src/common/utilities/imagery.py\u001b[0m in \u001b[0;36mwrite_array_to_tif\u001b[0;34m(data, dst_path, bbox, dtype, epsg, nodata, is_cog)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mrasterio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrite_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0mdst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mrasterio/_base.pyx\u001b[0m in \u001b[0;36mrasterio._base.DatasetBase.__exit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mrasterio/_base.pyx\u001b[0m in \u001b[0;36mrasterio._base.DatasetBase.close\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.10/contextlib.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreceived_exc\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msuppressed_exc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    583\u001b[0m         \u001b[0;34m\"\"\"Immediately unwind the context stack.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "from patchify import patchify\n",
    "import rasterio\n",
    "\n",
    "\n",
    "from common.constants import NODATA_BYTE, NODATA_FLOAT32\n",
    "import common.utilities.imagery as imagery\n",
    "import common.utilities.visualization as visualization\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "PATCH_SIZE = 512\n",
    "\n",
    "scene_dirs = glob.glob(f'./data/20*/S2*')\n",
    "\n",
    "for i, scene_dir in enumerate(scene_dirs):\n",
    "    \n",
    "    scene = scene_dir.split('/')[-1]\n",
    "    print(scene)\n",
    "    \n",
    "    with rasterio.open(f'{scene_dir}/B08.tif') as src:\n",
    "        if src.width < 512 or src.height < 512:\n",
    "            continue\n",
    "    \n",
    "    stack_data = []\n",
    "    transform = None\n",
    "    tif_paths = sorted(glob.glob(f'{scene_dir}/B*.tif'))\n",
    "    for path in tif_paths:\n",
    "        with rasterio.open(path) as src:\n",
    "            transform = src.transform\n",
    "            stack_data.append(src.read(1))\n",
    "            \n",
    "    stack_data = np.ma.array(stack_data)\n",
    "    stack_data = imagery.normalize_3d_array(stack_data).transpose((1, 2, 0))\n",
    "        \n",
    "    source_patches = patchify(stack_data, (PATCH_SIZE, PATCH_SIZE, stack_data.shape[2]), step=PATCH_SIZE)\n",
    "    \n",
    "    for irow in range(source_patches.shape[0]):\n",
    "        for icol in range(source_patches.shape[1]):\n",
    "            source_data = source_patches[irow, icol, 0, :, :, :]            \n",
    "            source_data = np.ma.array(source_data, mask=(source_data==NODATA_FLOAT32))\n",
    "            \n",
    "            if source_data.mask.sum() > 0:\n",
    "                print('skipping:', irow, icol)\n",
    "                continue\n",
    "                \n",
    "            #if irow < 3 or icol < 3:\n",
    "            #    continue\n",
    "\n",
    "            rgb_data = source_data[:, :, [2, 1, 0]]\n",
    "            rgb_data_norm = np.round(np.multiply(rgb_data, 254)).astype(int)\n",
    "            rgb_data_norm2 = imagery.normalize_0_254_3d_array(rgb_data)\n",
    "            \n",
    "            #print('\\t', np.min(rgb_data_norm), np.median(rgb_data_norm), np.max(rgb_data_norm))\n",
    "            #print('\\t', np.min(rgb_data_norm2), np.median(rgb_data_norm2), np.max(rgb_data_norm2))\n",
    "            #print('\\t-------------')\n",
    "            \n",
    "            #fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 12))\n",
    "            #visualization.plot_bands(rgb_data1, ax=ax1, bands=[0, 1, 2], transpose=False)\n",
    "            #visualization.plot_bands(rgb_data2, ax=ax2, bands=[0, 1, 2], transpose=False)\n",
    "            #raise\n",
    "                \n",
    "            source_path = f'{source_dir}/{scene}_{irow}_{icol}.tif'\n",
    "            rgb_path = f'{rgb_dir}/{scene}_{irow}_{icol}.tif'\n",
    "                    \n",
    "            x_min, y_min = rasterio.transform.xy(transform, PATCH_SIZE*(irow+1), PATCH_SIZE*icol)\n",
    "            x_max, y_max = rasterio.transform.xy(transform, PATCH_SIZE*irow, PATCH_SIZE*(icol+1))             \n",
    "            bbox = [x_min, y_min, x_max, y_max]\n",
    "            \n",
    "            imagery.write_array_to_tif(rgb_data_norm, rgb_path, bbox, dtype=np.uint8, nodata=255, is_cog=True) \n",
    "            imagery.write_array_to_tif(source_data, source_path, bbox, dtype=np.float32, nodata=NODATA_FLOAT32) \n",
    "                   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7734588f-1cff-45c5-9b5b-7e124bfae76b",
   "metadata": {},
   "source": [
    "## Create scene patches in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4bfb481c-1310-4d3a-8438-a907164ab392",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103 scenes\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import glob\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "from patchify import patchify\n",
    "import rasterio\n",
    "\n",
    "\n",
    "from common.constants import NODATA_BYTE, NODATA_FLOAT32\n",
    "import common.utilities.imagery as imagery\n",
    "import common.utilities.visualization as visualization\n",
    "\n",
    "\n",
    "\n",
    "PATCH_SIZE = 512\n",
    "\n",
    "\n",
    "def process_scene(scene_dir):\n",
    "    \n",
    "    scene = scene_dir.split('/')[-1]\n",
    "    \n",
    "    with rasterio.open(f'{scene_dir}/B08.tif') as src:\n",
    "        if src.width < 512 or src.height < 512:\n",
    "            return\n",
    "\n",
    "    stack_data = []\n",
    "    transform = None\n",
    "    tif_paths = sorted(glob.glob(f'{scene_dir}/B*.tif'))\n",
    "    for path in tif_paths:\n",
    "        with rasterio.open(path) as src:\n",
    "            transform = src.transform\n",
    "            stack_data.append(src.read(1))\n",
    "\n",
    "    stack_data = np.ma.array(stack_data)\n",
    "    stack_data = imagery.normalize_3d_array(stack_data).transpose((1, 2, 0))\n",
    "        \n",
    "    source_patches = patchify(stack_data, (PATCH_SIZE, PATCH_SIZE, stack_data.shape[2]), step=PATCH_SIZE)\n",
    "    \n",
    "    for irow in range(source_patches.shape[0]):\n",
    "        for icol in range(source_patches.shape[1]):\n",
    "            source_data = source_patches[irow, icol, 0, :, :, :]            \n",
    "            source_data = np.ma.array(source_data, mask=(source_data==NODATA_FLOAT32))\n",
    "            \n",
    "            if source_data.mask.sum() > 0:\n",
    "                print('skipping:', irow, icol)\n",
    "                continue\n",
    "                \n",
    "            rgb_data = source_data[:, :, [2, 1, 0]]\n",
    "            rgb_data_norm = np.round(np.multiply(rgb_data, 255)).astype(np.uint8) + 1\n",
    "            \n",
    "            source_path = f'{source_dir}/{scene}_{irow}_{icol}.tif'\n",
    "            rgb_path = f'{rgb_dir}/{scene}_{irow}_{icol}.tif'\n",
    "                    \n",
    "            x_min, y_min = rasterio.transform.xy(transform, PATCH_SIZE*(irow+1), PATCH_SIZE*icol)\n",
    "            x_max, y_max = rasterio.transform.xy(transform, PATCH_SIZE*irow, PATCH_SIZE*(icol+1))             \n",
    "            bbox = [x_min, y_min, x_max, y_max]\n",
    "            \n",
    "            imagery.write_array_to_tif(rgb_data_norm, rgb_path, bbox, dtype=np.uint8, nodata=0, is_cog=True) \n",
    "            imagery.write_array_to_tif(source_data, source_path, bbox, dtype=np.float32, nodata=NODATA_FLOAT32) \n",
    "                       \n",
    "            \n",
    "            \n",
    "scene_dirs = glob.glob(f'./data/20*/S2*')\n",
    "print(f'{len(scene_dirs)} scenes')\n",
    "with multiprocessing.Pool(processes=multiprocessing.cpu_count()) as pool:\n",
    "    patches = pool.map(process_scene, scene_dirs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fc12838c-0196-40be-a293-41f00bd47581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driver: GTiff/GeoTIFF\n",
      "Files: ./data/labelbox/clouds/rgb/S2A_35MQV_20200731_0_L2A_0_0.tif\n",
      "Size is 512, 512\n",
      "Coordinate System is:\n",
      "GEOGCRS[\"WGS 84\",\n",
      "    DATUM[\"World Geodetic System 1984\",\n",
      "        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n",
      "            LENGTHUNIT[\"metre\",1]]],\n",
      "    PRIMEM[\"Greenwich\",0,\n",
      "        ANGLEUNIT[\"degree\",0.0174532925199433]],\n",
      "    CS[ellipsoidal,2],\n",
      "        AXIS[\"geodetic latitude (Lat)\",north,\n",
      "            ORDER[1],\n",
      "            ANGLEUNIT[\"degree\",0.0174532925199433]],\n",
      "        AXIS[\"geodetic longitude (Lon)\",east,\n",
      "            ORDER[2],\n",
      "            ANGLEUNIT[\"degree\",0.0174532925199433]],\n",
      "    ID[\"EPSG\",4326]]\n",
      "Data axis to CRS axis mapping: 2,1\n",
      "Origin = (29.242784885946573,-0.591630233123690)\n",
      "Pixel Size = (0.000089771893148,-0.000090466247379)\n",
      "Metadata:\n",
      "  AREA_OR_POINT=Area\n",
      "Image Structure Metadata:\n",
      "  INTERLEAVE=PIXEL\n",
      "  LAYOUT=COG\n",
      "Corner Coordinates:\n",
      "Upper Left  (  29.2427849,  -0.5916302) ( 29d14'34.03\"E,  0d35'29.87\"S)\n",
      "Lower Left  (  29.2427849,  -0.6379490) ( 29d14'34.03\"E,  0d38'16.62\"S)\n",
      "Upper Right (  29.2887481,  -0.5916302) ( 29d17'19.49\"E,  0d35'29.87\"S)\n",
      "Lower Right (  29.2887481,  -0.6379490) ( 29d17'19.49\"E,  0d38'16.62\"S)\n",
      "Center      (  29.2657665,  -0.6147896) ( 29d15'56.76\"E,  0d36'53.24\"S)\n",
      "Band 1 Block=512x512 Type=Byte, ColorInterp=Red\n",
      "  Minimum=0.000, Maximum=254.000, Mean=53.907, StdDev=50.988\n",
      "  NoData Value=255\n",
      "  Metadata:\n",
      "    STATISTICS_MAXIMUM=254\n",
      "    STATISTICS_MEAN=53.906730651855\n",
      "    STATISTICS_MINIMUM=0\n",
      "    STATISTICS_STDDEV=50.988056638052\n",
      "    STATISTICS_VALID_PERCENT=100\n",
      "Band 2 Block=512x512 Type=Byte, ColorInterp=Green\n",
      "  Minimum=0.000, Maximum=254.000, Mean=52.718, StdDev=46.561\n",
      "  NoData Value=255\n",
      "  Metadata:\n",
      "    STATISTICS_MAXIMUM=254\n",
      "    STATISTICS_MEAN=52.717845916748\n",
      "    STATISTICS_MINIMUM=0\n",
      "    STATISTICS_STDDEV=46.561163657992\n",
      "    STATISTICS_VALID_PERCENT=100\n",
      "Band 3 Block=512x512 Type=Byte, ColorInterp=Blue\n",
      "  Minimum=0.000, Maximum=254.000, Mean=32.018, StdDev=46.706\n",
      "  NoData Value=255\n",
      "  Metadata:\n",
      "    STATISTICS_MAXIMUM=254\n",
      "    STATISTICS_MEAN=32.017772674561\n",
      "    STATISTICS_MINIMUM=0\n",
      "    STATISTICS_STDDEV=46.706124925424\n",
      "    STATISTICS_VALID_PERCENT=100\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "gdalinfo -stats ./data/labelbox/clouds/rgb/S2A_35MQV_20200731_0_L2A_0_0.tif\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ba8170-0aa0-41b2-a3b8-06a3244ce76e",
   "metadata": {},
   "source": [
    "## Upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3fe10813-9fc8-4c4b-a72c-55f59e30a670",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import common.constants as constants\n",
    "import common.aws.s3 as s3_utils\n",
    "\n",
    "\n",
    "def save_patch_to_s3(tif_path):\n",
    "    \n",
    "    file_name = tif_path.split('/')[-1]    \n",
    "    object_key = f'clouds/{file_name}'\n",
    "    href = f'https://data.smartcarte.earth/{object_key}'\n",
    "    # print(f'uploading {tif_path} to s3://{constants.S3_DATA_BUCKET}/{object_key}')\n",
    "    s3_utils.put_item(tif_path, constants.S3_DATA_BUCKET, object_key)\n",
    "    \n",
    "    return href\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5704a02d-15b2-427a-b183-37f1c6379fca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3633 paths\n",
      "3632 paths\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19/1900094115.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{len(rgb_paths)} paths'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import shelve\n",
    "\n",
    "\n",
    "state_path = \"./data/trainCloudsState\"\n",
    "with shelve.open(state_path) as state:\n",
    "\n",
    "    rgb_paths = glob.glob(f'{rgb_dir}/*.tif')\n",
    "    rgb_paths = [path for path in rgb_paths if not path.endswith(\"deg.tif\")]\n",
    "\n",
    "    for i, path in enumerate(rgb_paths):\n",
    "        \n",
    "        if i % 200 == 0:\n",
    "            print(f'{i} done')\n",
    "            \n",
    "        job_name = path.split('/')[-1].replace('.tif', '')\n",
    "        s3_href = save_patch_to_s3(path)\n",
    "        state[job_name] = {\n",
    "            'rgb_cog_href': s3_href\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a520fa-d58f-4c54-91ad-16e18b0b6d3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3360acef-f0e3-4f5d-8a9f-3465878ddc32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 data rows\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import glob\n",
    "import json\n",
    "from labelbox import Client, Dataset, DataRow\n",
    "import os\n",
    "import random\n",
    "import shelve\n",
    "from uuid import uuid4\n",
    "\n",
    "\n",
    "client = Client(api_key=LABELBOX_API_KEY)\n",
    "project = client.get_project(CLOUD_PROJECT_ID)\n",
    "\n",
    "today = datetime.datetime.today().strftime('%Y%m%d')\n",
    "\n",
    "clouds_dataset_name = f\"{today} Clouds\"\n",
    "\n",
    "clouds_dataset = client.get_datasets(where=(Dataset.name==clouds_dataset_name)).get_one()\n",
    "if clouds_dataset is None:\n",
    "    clouds_dataset = client.create_dataset(name=clouds_dataset_name)\n",
    "else:\n",
    "    raise ValueError(\"today's clouds dataset already exists\")\n",
    "\n",
    "    \n",
    "payload = []\n",
    "with shelve.open(state_path) as state:\n",
    "    for job_name in state.keys():        \n",
    "        row = {\n",
    "            \"min_zoom\": 12,\n",
    "            \"max_zoom\": 14,\n",
    "            \"tile_layer_url\": state[job_name][\"rgb_cog_href\"]\n",
    "        }\n",
    "        \n",
    "        payload.append(row)\n",
    "\n",
    "\n",
    "payload = [{DataRow.row_data: row} for row in payload]\n",
    "\n",
    "random.seed(666)\n",
    "random.shuffle(payload)\n",
    "\n",
    "start_idx, end_idx = 0, 1000\n",
    "payload = payload[start_idx:end_idx]\n",
    "\n",
    "print(f'{len(payload)} data rows')\n",
    "task = clouds_dataset.create_data_rows(payload)\n",
    "task.wait_till_done()\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5359cea1-9d95-401f-8b53-9d062bad39db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "664\n"
     ]
    }
   ],
   "source": [
    "files = glob.glob(\"./data/labelbox/clouds/source/*.tif\")\n",
    "print(len(files))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9805967c-93be-4b48-af14-ebc3640c176d",
   "metadata": {},
   "source": [
    "### Pull data from Labelbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5fc03763-531d-4dbb-ad54-049c80359562",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from labelbox import Client\n",
    "\n",
    "\n",
    "def get_data_rows():\n",
    "    client = Client(api_key=LABELBOX_API_KEY)\n",
    "    project = client.get_project(CLOUD_PROJECT_ID)\n",
    "\n",
    "    now = datetime.today()\n",
    "    then = now - timedelta(days=60)\n",
    "\n",
    "    data_rows = project.export_labels(download=True) #, start=then.strftime('%Y-%m-%d'), end=now.strftime('%Y-%m-%d'))    \n",
    "    return data_rows\n",
    "\n",
    "\n",
    "def is_island(target_polygon, polygon_list):\n",
    "    for curr_poly in polygon_list:\n",
    "        if target_polygon.within(curr_poly):\n",
    "            return True\n",
    "        \n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9784ca8e-e165-4c54-9564-b02df0c4e6ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 done data rows\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.features import geometry_mask\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "\n",
    "from common.utilities.imagery import write_array_to_tif\n",
    "from common.utilities.visualization import plot_bands\n",
    "\n",
    "\n",
    "CLOUD_CLASSES = {\n",
    "    'no_cloud': 0,\n",
    "    'cloud': 1\n",
    "}\n",
    "\n",
    "data_rows = get_data_rows()    \n",
    "data_rows = [dr for dr in data_rows if dr['DataRow Workflow Info']['taskName'] == \"Done\"]\n",
    "print(f'{len(data_rows)} done data rows')\n",
    "\n",
    "for i, row in enumerate(data_rows):\n",
    "        \n",
    "    row_id = row['DataRow ID']\n",
    "    labels = row['Label']['objects']\n",
    "    metadata = json.loads(row['Labeled Data'])\n",
    "    rgb_cog_url = metadata['tileLayerUrl']\n",
    "    job_name = rgb_cog_url.split('/')[-1].replace('.tif', '')\n",
    "    \n",
    "    rgb_path = f'{rgb_dir}/{job_name}.tif'\n",
    "    composite_path = f'{source_dir}/{job_name}.tif'\n",
    "    label_path = f'{labels_dir}/{job_name}.tif'\n",
    "\n",
    "    with rasterio.open(composite_path) as src:\n",
    "        bbox = list(src.bounds)\n",
    "        composite_shape = src.shape\n",
    "        composite_transform = src.transform\n",
    "    \n",
    "    label_polygons = {\n",
    "        label['featureId']: Polygon(label['geometry']['coordinates'][0])\n",
    "        for label in labels\n",
    "    }\n",
    "        \n",
    "    # 0: no_cloud, 1: cloud\n",
    "    labels_data = np.zeros(composite_shape).astype(np.uint8)\n",
    "    \n",
    "    islands = []    \n",
    "    sorted_labels = sorted(labels, key=lambda lab: lab['value'], reverse=True)   \n",
    "    for label in sorted_labels:\n",
    "        class_value = label['value']\n",
    "        class_idx = CLOUD_CLASSES[class_value]\n",
    "                \n",
    "        label_id = label['featureId']\n",
    "        polygon = label_polygons[label_id]\n",
    "        polygon_mask = geometry_mask([polygon], composite_shape, composite_transform, invert=True)\n",
    "        \n",
    "        test_polygons = label_polygons.copy()\n",
    "        del test_polygons[label_id]\n",
    "        is_poly_island = is_island(polygon, list(test_polygons.values()))\n",
    "                \n",
    "        if is_poly_island:\n",
    "            islands.append((class_idx, polygon_mask))\n",
    "        else:\n",
    "            labels_data[polygon_mask] = class_idx\n",
    "\n",
    "    \n",
    "    for class_idx, polygon_mask in islands:\n",
    "        labels_data[polygon_mask] = class_idx\n",
    "        \n",
    "    write_array_to_tif(labels_data, label_path, bbox, dtype=np.uint8, nodata=255)\n",
    "\n",
    "    # plot\n",
    "    if i == 36:\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 12))\n",
    "        with rasterio.open(rgb_path) as src:\n",
    "            data = src.read()\n",
    "            plot_bands(data, ax=ax1, bands=[0, 1, 2], transpose=True)\n",
    "\n",
    "        ax2.imshow(labels_data, vmin=0, vmax=1)\n",
    "       \n",
    "        \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2671b5f-07e3-43d1-948c-5b10c5d25fa7",
   "metadata": {},
   "source": [
    "## Data augmentation - rotate images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "feee6d5d-3cc0-4e5c-ac26-629cbbbb0e58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import rasterio\n",
    "\n",
    "from common.constants import NODATA_BYTE, NODATA_FLOAT32\n",
    "import common.utilities.imagery as imagery\n",
    "import common.utilities.visualization as visualization\n",
    "\n",
    "\n",
    "label_paths = glob.glob(f'{labels_dir}/S2*[0-9].tif')\n",
    "\n",
    "\n",
    "for label_path in label_paths:\n",
    "    rgb_path = label_path.replace(labels_dir, rgb_dir)\n",
    "    source_path = label_path.replace(labels_dir, source_dir)\n",
    "        \n",
    "    with rasterio.open(source_path) as source_src:\n",
    "        source_data = source_src.read()\n",
    "        bbox = list(source_src.bounds)\n",
    "        \n",
    "    with rasterio.open(rgb_path) as rgb_src:\n",
    "        rgb_data = rgb_src.read()\n",
    "        \n",
    "    with rasterio.open(label_path) as label_src:\n",
    "        label_data = label_src.read(1)\n",
    "        \n",
    "        \n",
    "    rgb_target_path = label_path.replace('/labels/', '/rgb/')\n",
    "    source_target_path = label_path.replace('/labels/', '/source/')\n",
    "\n",
    "    imagery.write_array_to_tif(rgb_data.transpose((1, 2, 0)), rgb_target_path, bbox, dtype=np.float32, nodata=NODATA_FLOAT32)\n",
    "    imagery.write_array_to_tif(source_data.transpose((1, 2, 0)), source_target_path, bbox, dtype=np.float32, nodata=NODATA_FLOAT32)\n",
    "                    \n",
    "    for i in range(1, 4):\n",
    "        \n",
    "        source_rotated = np.rot90(source_data, k=i, axes=(1, 2))\n",
    "        source_rotated = source_rotated.transpose((1, 2, 0))\n",
    "        \n",
    "        rgb_rotated = np.rot90(rgb_data, k=i, axes=(1, 2))\n",
    "        rgb_rotated = rgb_rotated.transpose((1, 2, 0))\n",
    "        \n",
    "        label_rotated = np.rot90(label_data, k=i, axes=(0, 1))\n",
    "        \n",
    "        source_rotated_path = source_target_path.replace('.tif', f'_{i*90}deg.tif')\n",
    "        rgb_rotated_path = rgb_target_path.replace('.tif', f'_{i*90}deg.tif')\n",
    "        label_rotated_path = label_path.replace('.tif', f'_{i*90}deg.tif')\n",
    "                \n",
    "        imagery.write_array_to_tif(source_rotated, source_rotated_path, bbox, dtype=np.float32, nodata=NODATA_FLOAT32)\n",
    "        imagery.write_array_to_tif(rgb_rotated, rgb_rotated_path, bbox, dtype=np.float32, nodata=NODATA_FLOAT32)\n",
    "        imagery.write_array_to_tif(label_rotated, label_rotated_path, bbox, dtype=np.uint8, nodata=NODATA_BYTE)\n",
    "        \n",
    "        #fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 12))\n",
    "        #plot_bands(rgb_data, ax=ax1, bands=[0, 1, 2], transpose=True)\n",
    "        #plot_bands(rgb_rotated, ax=ax2, bands=[0, 1, 2], transpose=False)\n",
    "        #ax1.imshow(label_data, vmin=0, vmax=1)\n",
    "        #ax2.imshow(label_rotated, vmin=0, vmax=1)        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4592be9d-9dbf-4e23-8237-b9b159cd548f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create train and val folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "44018b2f-ba6c-441d-a3ad-c02fa4aca587",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Copying files: 0 files [00:00, ? files/s]\u001b[A\n",
      "Copying files: 2 files [00:00, 17.10 files/s]\u001b[A\n",
      "Copying files: 4 files [00:00, 13.24 files/s]\u001b[A\n",
      "Copying files: 6 files [00:00, 12.10 files/s]\u001b[A\n",
      "Copying files: 8 files [00:00, 11.91 files/s]\u001b[A\n",
      "Copying files: 10 files [00:00, 11.87 files/s]\u001b[A\n",
      "Copying files: 12 files [00:00, 11.79 files/s]\u001b[A\n",
      "Copying files: 14 files [00:01, 11.83 files/s]\u001b[A\n",
      "Copying files: 16 files [00:01, 11.41 files/s]\u001b[A\n",
      "Copying files: 18 files [00:01, 11.44 files/s]\u001b[A\n",
      "Copying files: 20 files [00:01, 11.27 files/s]\u001b[A\n",
      "Copying files: 22 files [00:01, 11.29 files/s]\u001b[A\n",
      "Copying files: 24 files [00:02, 11.40 files/s]\u001b[A\n",
      "Copying files: 26 files [00:02, 11.52 files/s]\u001b[A\n",
      "Copying files: 28 files [00:02, 11.41 files/s]\u001b[A\n",
      "Copying files: 30 files [00:02, 11.41 files/s]\u001b[A\n",
      "Copying files: 32 files [00:02, 11.38 files/s]\u001b[A\n",
      "Copying files: 34 files [00:02, 11.49 files/s]\u001b[A\n",
      "Copying files: 36 files [00:03, 11.85 files/s]\u001b[A\n",
      "Copying files: 38 files [00:03, 11.74 files/s]\u001b[A\n",
      "Copying files: 40 files [00:03, 11.38 files/s]\u001b[A\n",
      "Copying files: 42 files [00:03, 11.18 files/s]\u001b[A\n",
      "Copying files: 44 files [00:03, 11.32 files/s]\u001b[A\n",
      "Copying files: 46 files [00:03, 11.30 files/s]\u001b[A\n",
      "Copying files: 48 files [00:04, 11.52 files/s]\u001b[A\n",
      "Copying files: 50 files [00:04, 11.60 files/s]\u001b[A\n",
      "Copying files: 52 files [00:04, 11.50 files/s]\u001b[A\n",
      "Copying files: 54 files [00:04, 11.40 files/s]\u001b[A\n",
      "Copying files: 56 files [00:04, 11.32 files/s]\u001b[A\n",
      "Copying files: 58 files [00:05, 11.21 files/s]\u001b[A\n",
      "Copying files: 60 files [00:05, 11.25 files/s]\u001b[A\n",
      "Copying files: 62 files [00:05, 11.27 files/s]\u001b[A\n",
      "Copying files: 64 files [00:05, 11.52 files/s]\u001b[A\n",
      "Copying files: 66 files [00:05, 11.57 files/s]\u001b[A\n",
      "Copying files: 68 files [00:05, 11.72 files/s]\u001b[A\n",
      "Copying files: 70 files [00:06, 11.55 files/s]\u001b[A\n",
      "Copying files: 72 files [00:06, 11.68 files/s]\u001b[A\n",
      "Copying files: 74 files [00:06, 11.68 files/s]\u001b[A\n",
      "Copying files: 76 files [00:06, 11.75 files/s]\u001b[A\n",
      "Copying files: 78 files [00:06, 11.51 files/s]\u001b[A\n",
      "Copying files: 80 files [00:06, 11.71 files/s]\u001b[A\n",
      "Copying files: 82 files [00:07, 11.78 files/s]\u001b[A\n",
      "Copying files: 84 files [00:07, 11.60 files/s]\u001b[A\n",
      "Copying files: 86 files [00:07, 11.56 files/s]\u001b[A\n",
      "Copying files: 88 files [00:07, 10.96 files/s]\u001b[A\n",
      "Copying files: 90 files [00:07, 11.32 files/s]\u001b[A\n",
      "Copying files: 92 files [00:07, 11.44 files/s]\u001b[A\n",
      "Copying files: 94 files [00:08, 11.35 files/s]\u001b[A\n",
      "Copying files: 96 files [00:08, 11.48 files/s]\u001b[A\n",
      "Copying files: 98 files [00:08, 11.27 files/s]\u001b[A\n",
      "Copying files: 100 files [00:08, 11.19 files/s]\u001b[A\n",
      "Copying files: 102 files [00:08, 11.38 files/s]\u001b[A\n",
      "Copying files: 104 files [00:09, 11.30 files/s]\u001b[A\n",
      "Copying files: 106 files [00:09, 10.81 files/s]\u001b[A\n",
      "Copying files: 108 files [00:09, 10.99 files/s]\u001b[A\n",
      "Copying files: 110 files [00:09, 11.13 files/s]\u001b[A\n",
      "Copying files: 112 files [00:09, 11.36 files/s]\u001b[A\n",
      "Copying files: 114 files [00:09, 11.51 files/s]\u001b[A\n",
      "Copying files: 116 files [00:10, 11.63 files/s]\u001b[A\n",
      "Copying files: 118 files [00:10, 11.52 files/s]\u001b[A\n",
      "Copying files: 120 files [00:10, 11.53 files/s]\u001b[A\n",
      "Copying files: 122 files [00:10, 11.52 files/s]\u001b[A\n",
      "Copying files: 124 files [00:10, 11.07 files/s]\u001b[A\n",
      "Copying files: 126 files [00:10, 11.35 files/s]\u001b[A\n",
      "Copying files: 128 files [00:11, 11.50 files/s]\u001b[A\n",
      "Copying files: 130 files [00:11, 11.46 files/s]\u001b[A\n",
      "Copying files: 132 files [00:11, 11.40 files/s]\u001b[A\n",
      "Copying files: 134 files [00:11, 11.35 files/s]\u001b[A\n",
      "Copying files: 136 files [00:11, 11.49 files/s]\u001b[A\n",
      "Copying files: 138 files [00:12, 11.79 files/s]\u001b[A\n",
      "Copying files: 140 files [00:12, 11.79 files/s]\u001b[A\n",
      "Copying files: 142 files [00:12, 11.90 files/s]\u001b[A\n",
      "Copying files: 144 files [00:12, 11.76 files/s]\u001b[A\n",
      "Copying files: 146 files [00:12, 12.62 files/s]\u001b[A\n",
      "Copying files: 149 files [00:12, 16.31 files/s]\u001b[A\n",
      "Copying files: 153 files [00:12, 19.75 files/s]\u001b[A\n",
      "Copying files: 156 files [00:12, 21.94 files/s]\u001b[A\n",
      "Copying files: 160 files [00:13, 24.92 files/s]\u001b[A\n",
      "Copying files: 163 files [00:13, 26.13 files/s]\u001b[A\n",
      "Copying files: 167 files [00:13, 27.83 files/s]\u001b[A\n",
      "Copying files: 170 files [00:13, 28.39 files/s]\u001b[A\n",
      "Copying files: 173 files [00:13, 28.31 files/s]\u001b[A\n",
      "Copying files: 176 files [00:13, 28.74 files/s]\u001b[A\n",
      "Copying files: 180 files [00:13, 29.87 files/s]\u001b[A\n",
      "Copying files: 184 files [00:13, 30.63 files/s]\u001b[A\n",
      "Copying files: 188 files [00:14, 30.74 files/s]\u001b[A\n",
      "Copying files: 192 files [00:14, 30.87 files/s]\u001b[A\n",
      "Copying files: 196 files [00:14, 30.48 files/s]\u001b[A\n",
      "Copying files: 200 files [00:14, 30.39 files/s]\u001b[A\n",
      "Copying files: 204 files [00:14, 30.42 files/s]\u001b[A\n",
      "Copying files: 208 files [00:14, 31.02 files/s]\u001b[A\n",
      "Copying files: 212 files [00:14, 31.74 files/s]\u001b[A\n",
      "Copying files: 216 files [00:14, 31.33 files/s]\u001b[A\n",
      "Copying files: 220 files [00:15, 31.15 files/s]\u001b[A\n",
      "Copying files: 224 files [00:15, 30.80 files/s]\u001b[A\n",
      "Copying files: 228 files [00:15, 31.44 files/s]\u001b[A\n",
      "Copying files: 232 files [00:15, 31.53 files/s]\u001b[A\n",
      "Copying files: 236 files [00:15, 31.21 files/s]\u001b[A\n",
      "Copying files: 240 files [00:15, 31.49 files/s]\u001b[A\n",
      "Copying files: 244 files [00:15, 31.62 files/s]\u001b[A\n",
      "Copying files: 248 files [00:15, 31.09 files/s]\u001b[A\n",
      "Copying files: 252 files [00:16, 31.12 files/s]\u001b[A\n",
      "Copying files: 256 files [00:16, 30.98 files/s]\u001b[A\n",
      "Copying files: 260 files [00:16, 30.10 files/s]\u001b[A\n",
      "Copying files: 264 files [00:16, 30.22 files/s]\u001b[A\n",
      "Copying files: 268 files [00:16, 29.93 files/s]\u001b[A\n",
      "Copying files: 271 files [00:16, 29.12 files/s]\u001b[A\n",
      "Copying files: 274 files [00:16, 29.30 files/s]\u001b[A\n",
      "Copying files: 278 files [00:16, 30.19 files/s]\u001b[A\n",
      "Copying files: 282 files [00:17, 29.77 files/s]\u001b[A\n",
      "Copying files: 285 files [00:17, 29.75 files/s]\u001b[A\n",
      "Copying files: 289 files [00:17, 29.50 files/s]\u001b[A\n",
      "Copying files: 292 files [00:17, 21.54 files/s]\u001b[A\n",
      "Copying files: 295 files [00:17, 18.53 files/s]\u001b[A\n",
      "Copying files: 298 files [00:18, 16.08 files/s]\u001b[A\n",
      "Copying files: 300 files [00:18, 15.29 files/s]\u001b[A\n",
      "Copying files: 302 files [00:18, 14.75 files/s]\u001b[A\n",
      "Copying files: 304 files [00:18, 13.51 files/s]\u001b[A\n",
      "Copying files: 306 files [00:18, 13.55 files/s]\u001b[A\n",
      "Copying files: 308 files [00:18, 13.32 files/s]\u001b[A\n",
      "Copying files: 310 files [00:19, 12.89 files/s]\u001b[A\n",
      "Copying files: 312 files [00:19, 12.73 files/s]\u001b[A\n",
      "Copying files: 314 files [00:19, 12.94 files/s]\u001b[A\n",
      "Copying files: 316 files [00:19, 12.93 files/s]\u001b[A\n",
      "Copying files: 318 files [00:19, 12.66 files/s]\u001b[A\n",
      "Copying files: 320 files [00:19, 12.58 files/s]\u001b[A\n",
      "Copying files: 322 files [00:19, 12.30 files/s]\u001b[A\n",
      "Copying files: 324 files [00:20, 12.75 files/s]\u001b[A\n",
      "Copying files: 326 files [00:20, 12.93 files/s]\u001b[A\n",
      "Copying files: 328 files [00:20, 13.08 files/s]\u001b[A\n",
      "Copying files: 330 files [00:20, 12.85 files/s]\u001b[A\n",
      "Copying files: 332 files [00:20, 12.85 files/s]\u001b[A\n",
      "Copying files: 334 files [00:20, 12.49 files/s]\u001b[A\n",
      "Copying files: 336 files [00:21, 12.69 files/s]\u001b[A\n",
      "Copying files: 338 files [00:21, 12.80 files/s]\u001b[A\n",
      "Copying files: 340 files [00:21, 12.95 files/s]\u001b[A\n",
      "Copying files: 342 files [00:21, 13.02 files/s]\u001b[A\n",
      "Copying files: 344 files [00:21, 13.25 files/s]\u001b[A\n",
      "Copying files: 346 files [00:21, 13.19 files/s]\u001b[A\n",
      "Copying files: 348 files [00:21, 13.11 files/s]\u001b[A\n",
      "Copying files: 350 files [00:22, 13.11 files/s]\u001b[A\n",
      "Copying files: 352 files [00:22, 13.05 files/s]\u001b[A\n",
      "Copying files: 354 files [00:22, 13.22 files/s]\u001b[A\n",
      "Copying files: 356 files [00:22, 12.82 files/s]\u001b[A\n",
      "Copying files: 358 files [00:22, 12.75 files/s]\u001b[A\n",
      "Copying files: 360 files [00:22, 12.89 files/s]\u001b[A\n",
      "Copying files: 362 files [00:23, 12.88 files/s]\u001b[A\n",
      "Copying files: 364 files [00:23, 12.81 files/s]\u001b[A\n",
      "Copying files: 366 files [00:23, 12.85 files/s]\u001b[A\n",
      "Copying files: 368 files [00:23, 12.89 files/s]\u001b[A\n",
      "Copying files: 370 files [00:23, 12.94 files/s]\u001b[A\n",
      "Copying files: 372 files [00:23, 13.05 files/s]\u001b[A\n",
      "Copying files: 374 files [00:23, 13.12 files/s]\u001b[A\n",
      "Copying files: 376 files [00:24, 12.91 files/s]\u001b[A\n",
      "Copying files: 378 files [00:24, 13.03 files/s]\u001b[A\n",
      "Copying files: 380 files [00:24, 13.11 files/s]\u001b[A\n",
      "Copying files: 382 files [00:24, 13.14 files/s]\u001b[A\n",
      "Copying files: 384 files [00:24, 13.09 files/s]\u001b[A\n",
      "Copying files: 386 files [00:24, 13.23 files/s]\u001b[A\n",
      "Copying files: 388 files [00:25, 13.19 files/s]\u001b[A\n",
      "Copying files: 390 files [00:25, 13.16 files/s]\u001b[A\n",
      "Copying files: 392 files [00:25, 13.20 files/s]\u001b[A\n",
      "Copying files: 394 files [00:25, 11.67 files/s]\u001b[A\n",
      "Copying files: 396 files [00:25, 11.33 files/s]\u001b[A\n",
      "Copying files: 398 files [00:25, 11.44 files/s]\u001b[A\n",
      "Copying files: 400 files [00:26, 11.84 files/s]\u001b[A\n",
      "Copying files: 402 files [00:26, 12.09 files/s]\u001b[A\n",
      "Copying files: 404 files [00:26, 12.30 files/s]\u001b[A\n",
      "Copying files: 406 files [00:26, 12.69 files/s]\u001b[A\n",
      "Copying files: 408 files [00:26, 12.80 files/s]\u001b[A\n",
      "Copying files: 410 files [00:26, 12.65 files/s]\u001b[A\n",
      "Copying files: 412 files [00:27, 12.70 files/s]\u001b[A\n",
      "Copying files: 414 files [00:27, 12.77 files/s]\u001b[A\n",
      "Copying files: 416 files [00:27, 12.74 files/s]\u001b[A\n",
      "Copying files: 418 files [00:27, 12.66 files/s]\u001b[A\n",
      "Copying files: 420 files [00:27, 11.88 files/s]\u001b[A\n",
      "Copying files: 422 files [00:27, 12.30 files/s]\u001b[A\n",
      "Copying files: 424 files [00:27, 12.54 files/s]\u001b[A\n",
      "Copying files: 426 files [00:28, 12.67 files/s]\u001b[A\n",
      "Copying files: 428 files [00:28, 12.65 files/s]\u001b[A\n",
      "Copying files: 430 files [00:28, 12.70 files/s]\u001b[A\n",
      "Copying files: 432 files [00:28, 15.04 files/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import splitfolders\n",
    "\n",
    "\n",
    "output_dir = \"./data/trainingCloudData\"\n",
    "\n",
    "if os.path.exists(output_dir):\n",
    "    shutil.rmtree(output_dir)\n",
    "\n",
    "splitfolders.ratio(training_dir, output=output_dir, seed=314, ratio=(.85, .15), group_prefix=None)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23bc0a7-ae55-4bc7-91c9-0919eb3051b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.4xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Geospatial 1.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:081189585635:image/sagemaker-geospatial-v1-0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
